{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "plt.style.use('dark_background')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"\n",
    "    initialize the grid world environemnt\n",
    "    \"\"\"\n",
    "    def __init__(self, height=7, width=7):\n",
    "        \n",
    "        # setting up the grid world\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.rewards = np.zeros((self.height, self.width)) - 1\n",
    "        self.rewards[0,0] = 20\n",
    "        \n",
    "        # set random start state (or location) for the agent\n",
    "        self.current_state = (self.height-1, 0)\n",
    "        \n",
    "        # set states (or locations) for the bomb and the gold\n",
    "        self.gold_state = (0,0)\n",
    "        self.terminal_states = [self.gold_state]\n",
    "        self.obstacle = [(2,0),(2,1),(2,2),(2,3),(2,4),(2,5)]\n",
    "        # set grid rewards for bomb and gold states\n",
    "        \n",
    "        # the set of available actions\n",
    "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']   \n",
    "        \n",
    "    def get_actions(self):\n",
    "        \"\"\"\n",
    "        return the set of actions (A)\n",
    "        \"\"\"\n",
    "        return self.actions\n",
    "    \n",
    "    def get_agent_location(self):\n",
    "        \"\"\"\n",
    "        returns the current location of the agent marked on the grid world as ``1''\n",
    "        \"\"\"\n",
    "        grid_world = np.zeros((self.height, self.width))\n",
    "        grid_world[self.current_state[0], self.current_state[1]] = 1\n",
    "        return grid_world\n",
    "    \n",
    "    def get_reward(self, state):\n",
    "        \"\"\"\n",
    "        returns the reward for the input state\n",
    "        \"\"\"\n",
    "        return self.rewards[state[0], state[1]]\n",
    "        \n",
    "    \n",
    "    def delta(self, action):\n",
    "        \"\"\"\n",
    "        moves the agent in the specified direction: if agent is at a border then it stays still\n",
    "        but takes negative reward and the function returns the immediate reward for the action\n",
    "        \"\"\"\n",
    "        # store the location before taking action\n",
    "        last_state = self.current_state\n",
    "        \n",
    "        # move UP\n",
    "        if action == 'UP':\n",
    "            # if the agent is at the top border, stay still, collect negative reward\n",
    "            new_pos = (last_state[0]-1,last_state[1])\n",
    "            if new_pos in self.obstacle:\n",
    "                reward = self.get_reward(last_state)\n",
    "            elif last_state[0] == 0:\n",
    "                reward = self.get_reward(last_state)\n",
    "            else:\n",
    "                self.current_state = (self.current_state[0] - 1, self.current_state[1])\n",
    "                reward = self.get_reward(self.current_state)\n",
    "                \n",
    "        # move DOWN\n",
    "        elif action == 'DOWN':\n",
    "            new_pos = (last_state[0] +1,last_state[1])\n",
    "            # if the agent is at the bottom border, stay still, collect negative reward\n",
    "            if new_pos in self.obstacle:\n",
    "                reward = self.get_reward(last_state)\n",
    "            elif last_state[0] == self.height - 1:\n",
    "                reward = self.get_reward(last_state)\n",
    "            else:\n",
    "                self.current_state = (self.current_state[0] + 1, self.current_state[1])\n",
    "                reward = self.get_reward(self.current_state)\n",
    "            \n",
    "        # move LEFT\n",
    "        elif action == 'LEFT':\n",
    "            new_pos = (last_state[0],last_state[1]-1)\n",
    "\n",
    "            # if the agent is at the left border, stay still, collect negative reward\n",
    "            if new_pos in self.obstacle:\n",
    "                reward = self.get_reward(last_state)\n",
    "            elif last_state[1] == 0:\n",
    "                reward = self.get_reward(last_state)\n",
    "            else:\n",
    "                self.current_state = (self.current_state[0], self.current_state[1] - 1)\n",
    "                reward = self.get_reward(self.current_state)\n",
    "\n",
    "        # move RIGHT\n",
    "        elif action == 'RIGHT':\n",
    "            # of the agent is at the right border, stay still, collect negative reward\n",
    "            new_pos = (last_state[0],last_state[1]+1)\n",
    "            if new_pos in self.obstacle:\n",
    "                reward = self.get_reward(last_state)\n",
    "            elif last_state[1] == self.width - 1:\n",
    "                reward = self.get_reward(last_state)\n",
    "            else:\n",
    "                self.current_state = (self.current_state[0], self.current_state[1] + 1)\n",
    "                reward = self.get_reward(self.current_state)\n",
    "\n",
    "        return reward\n",
    "    \n",
    "    def check_state(self):\n",
    "        \"\"\"\n",
    "        check if the agent is in a terminal state (gold or bomb):\n",
    "        if so return 'TERMINAL', otherwise return 'NONTERMINAL'\n",
    "        \"\"\"\n",
    "        current_state_type = 'NONTERMINAL'\n",
    "        if self.current_state in self.terminal_states:\n",
    "            current_state_type ='TERMINAL'\n",
    "        \n",
    "        return current_state_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self):\n",
    "        np.random.seed(7)\n",
    "        self.time = -1\n",
    "    # select a random action\n",
    "    def select_action(self, actions,state):\n",
    "        \"\"\"\n",
    "        returns a random action\n",
    "        \"\"\"\n",
    "        self.time += 1 \n",
    "        return np.random.choice(actions)  \n",
    "    def reset(self):\n",
    "        self.time = -1\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimalPath:\n",
    "    def __init__(self):\n",
    "        self.choices = ['RIGHT','RIGHT','RIGHT','RIGHT','RIGHT','RIGHT','UP','UP','UP','UP','UP','UP','LEFT','LEFT','LEFT','LEFT','LEFT','LEFT']\n",
    "        self.time = -1\n",
    "    # select a random action\n",
    "    def select_action(self, actions,state):\n",
    "        \"\"\"\n",
    "        returns a random action\n",
    "        \"\"\"\n",
    "        self.time += 1\n",
    "        return self.choices[self.time]\n",
    "    def reset(self):\n",
    "        self.time = -1\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgentDet:\n",
    "    def __init__(self, epsilon=0, gamma=1):\n",
    "        \"\"\"\n",
    "        initialises the Deterministic Q-Learning Agent\n",
    "        \"\"\"\n",
    "        self.time = -1\n",
    "        np.random.seed(7)\n",
    "        qs = [[20., 19., 18., 17, 16, 15, 14],\n",
    "       [19, 18., 17, 16, 15, 14, 13],\n",
    "       [ 0.,  0.,  0.,  0.,  0.,  0., 12],\n",
    "       [5, 6, 7, 8, 9, 10, 11],\n",
    "       [4, 5, 6, 7, 8, 9, 10],\n",
    "       [3, 4, 5, 6, 7, 8, 9],\n",
    "       [2, 3, 4, 5, 6, 7, 8]]\n",
    "        self.Qs = np.pad(qs, 1, mode='constant')\n",
    "        self.Q = dict() # store all Q-values in dictionary of dictionaries \n",
    "        for x in range(7): # loop through all possible grid spaces, create sub-dictionary for each grid\n",
    "            for y in range(7):\n",
    "                self.Q[(x,y)] = {'UP':self.Qs[(x,y+1)], 'DOWN':self.Qs[(x+2,y+1)], 'LEFT':self.Qs[(x+1,y)], 'RIGHT':self.Qs[(x+1,y+2)]} # populate sub-dictionary with zero values for possible moves\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "    def select_action(self, actions,current_state):\n",
    "        \"\"\"\n",
    "        returns the optimal action from the Q-table. if multiple optimal actions, chooses random choice.\n",
    "        will make an exploratory random action dependent on epsilon.\n",
    "        \"\"\"\n",
    "        self.time += 1\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = actions[np.random.randint(0, len(actions))]\n",
    "        else:\n",
    "            Q_state = self.Q[current_state]\n",
    "            Q_state_max = max(Q_state.values())\n",
    "            action = np.random.choice([k for k, v in Q_state.items() if v == Q_state_max])            \n",
    "        return action\n",
    "    \n",
    "    def update(self, current_state, reward, new_state, action):\n",
    "        \"\"\"\n",
    "        updates the Q-values using Q-learning\n",
    "        \"\"\"\n",
    "        Q_new_state = self.Q[new_state]        \n",
    "        Q_new_state_max = max(Q_new_state.values())\n",
    "        self.Q[current_state][action] = reward + self.gamma * Q_new_state_max\n",
    "        \n",
    "    def reset(self):\n",
    "        self.time = -1\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3.0],\n",
       " [(6, 0),\n",
       "  (6, 1),\n",
       "  (6, 2),\n",
       "  (5, 2),\n",
       "  (5, 3),\n",
       "  (4, 3),\n",
       "  (3, 3),\n",
       "  (3, 4),\n",
       "  (3, 5),\n",
       "  (3, 6),\n",
       "  (2, 6),\n",
       "  (1, 6),\n",
       "  (1, 5),\n",
       "  (1, 4),\n",
       "  (0, 4),\n",
       "  (0, 3),\n",
       "  (0, 2),\n",
       "  (0, 1),\n",
       "  (0, 0)])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def learn(agent, gamma=1.0, num_episodes=1, max_steps_per_episode=50, update_Q=False):\n",
    "    \"\"\"\n",
    "    the learn function plays episodes and updates Q-values\n",
    "    \"\"\"\n",
    "    np.random.seed(7)\n",
    "    path = []\n",
    "    reward_per_episode = [] # initialise performance log    \n",
    "    for episode in range(num_episodes): # run episodes\n",
    "        environment = Environment()\n",
    "        cumulative_reward = 0 # initialise cumulative reward of each episode\n",
    "        step = 0\n",
    "        episode_over = False\n",
    "        path = [(6,0)]\n",
    "        while step < max_steps_per_episode and episode_over != True: # run until max steps or until episode is over\n",
    "            current_state = environment.current_state\n",
    "            action = agent.select_action(environment.actions,current_state) \n",
    "            reward = environment.delta(action)\n",
    "            new_state = environment.current_state\n",
    "            path.append(new_state)\n",
    "            if update_Q == True: # update Q-values if learning is specified\n",
    "                agent.update(current_state, reward, new_state, action)\n",
    "                \n",
    "            cumulative_reward += reward * (gamma**step)\n",
    "            step += 1\n",
    "            if environment.check_state() == 'TERMINAL': # if agent is in terminal state, the episode is over and start the next episode\n",
    "                agent.reset()\n",
    "                environment.__init__()\n",
    "                episode_over = True\n",
    "        reward_per_episode.append(cumulative_reward) # Append reward for current trial to performance log\n",
    "        \n",
    "    return reward_per_episode,path # return episode log\n",
    "learn(QLearningAgentDet(),num_episodes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal rewards:  3.0 [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0] \n",
      " random rewards:  -50.0 [-50.0, -50.0, -50.0, -50.0, -50.0, -50.0, -50.0, -50.0, -50.0, -50.0, -50.0, -50.0, -50.0, -50.0, -50.0, -50.0, -50.0, -50.0, -50.0, -50.0] \n",
      " greedy rewards:  3.0 [3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Optimal</th>\n",
       "      <th>Random</th>\n",
       "      <th>Greedy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Optimal  Random  Greedy\n",
       "0      3.0   -50.0     3.0\n",
       "1      3.0   -50.0     3.0\n",
       "2      3.0   -50.0     3.0\n",
       "3      3.0   -50.0     3.0\n",
       "4      3.0   -50.0     3.0"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_agent = QLearningAgentDet()\n",
    "ran_agent = RandomAgent()\n",
    "opt_agent = optimalPath()\n",
    "iters  = 20\n",
    "optimal_episode,opt_path = learn( opt_agent, num_episodes = iters)\n",
    "ran_episode,ran_path = learn( ran_agent, num_episodes = iters)\n",
    "greedy_episode, gre_path = learn(greedy_agent,num_episodes = iters)\n",
    "print('optimal rewards: ',sum(optimal_episode)/20,optimal_episode,'\\n random rewards: ',sum(ran_episode)/20,ran_episode,'\\n greedy rewards: ',sum(greedy_episode)/20,greedy_episode)\n",
    "rewards_df = pd.DataFrame(columns = ['Optimal','Random','Greedy'])\n",
    "rewards_df['Optimal'] = optimal_episode\n",
    "rewards_df['Random'] = ran_episode\n",
    "rewards_df['Greedy'] = greedy_episode\n",
    "rewards_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Average Acc Return')"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure(figsize = (12,12))\n",
    "sns.barplot(data = rewards_df ).set_title('Average Acc Return')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 0), (6, 1), (5, 1), (6, 1), (6, 0), (6, 1), (6, 2), (6, 3), (6, 4), (5, 4), (6, 4), (6, 3), (6, 4), (5, 4), (6, 4), (6, 3), (6, 2), (6, 1), (5, 1), (5, 2), (5, 1), (4, 1), (3, 1), (3, 2), (3, 3), (3, 3), (3, 3), (3, 4), (3, 5), (3, 4), (3, 3), (3, 4), (3, 4), (3, 4), (3, 5), (4, 5), (3, 5), (3, 5), (3, 6), (4, 6), (3, 6), (3, 6), (4, 6), (3, 6), (3, 5), (3, 5), (4, 5), (4, 4), (4, 3), (3, 3), (3, 2)]\n"
     ]
    }
   ],
   "source": [
    "print(ran_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    height = 700\n",
    "    width = 700\n",
    "    image = Image.new(mode='RGB', size=(height, width), color=(255,255,255))\n",
    "\n",
    "    # Draw some lines\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    y_start = 0\n",
    "    y_end = image.height\n",
    "    step_size = int(image.width / 7)\n",
    "\n",
    "    for x in range(0, image.width, step_size):\n",
    "        line = ((x, y_start), (x, y_end))\n",
    "        draw.line(line, fill=128)\n",
    "\n",
    "    x_start = 0\n",
    "    x_end = image.width\n",
    "\n",
    "# draw text, half opacity\n",
    "\n",
    "    for y in range(0, image.height, step_size):\n",
    "        line = ((x_start, y), (x_end, y))\n",
    "        draw.line(line, fill=128)\n",
    "    for i,state in enumerate(opt_path[:-1]):\n",
    "        line = ((state[1]*100+50,state[0]*100+50),(opt_path[i+1][1]*100+50,opt_path[i+1][0]*100+50))\n",
    "        draw.line(line,fill='red',width = 10)\n",
    "        draw.text((state[1]*100+i,state[0]*100+20+i),str(i),fill='red')\n",
    "    for i,state in enumerate(gre_path[:-1]):\n",
    "        line = ((state[1]*100+40,state[0]*100+40),(gre_path[i+1][1]*100+40,gre_path[i+1][0]*100+40))\n",
    "        draw.line(line,fill='green',width = 10)\n",
    "        draw.text((state[1]*100+10+i,state[0]*100+10+i),str(i),fill='green')\n",
    "    for i,state in enumerate(ran_path[:-1]):\n",
    "        line = ((state[1]*100+30,state[0]*100+30),(ran_path[i+1][1]*100+30,ran_path[i+1][0]*100+30))\n",
    "        draw.line(line,fill='blue',width = 5)\n",
    "        ii = str(i)+' '\n",
    "        draw.text((state[1]*100,state[0]*100),ii,fill = 'blue')\n",
    "    #draw.line(((0,0),(600,600)),fill=128,width = 10)\n",
    "    draw.line(((0,250),(600,250)),fill = 'black',width = 100)\n",
    "    draw.text((0,0),'Optimal Path',fill='red')\n",
    "    draw.text((0,20),'Random Path',fill='blue')\n",
    "    draw.text((0,40),'Greedy Path',fill='green')\n",
    "    del draw\n",
    "\n",
    "    image.save('Path_Taken.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Creates a new image with the given mode and size.\n",
       "\n",
       ":param mode: The mode to use for the new image. See:\n",
       "   :ref:`concept-modes`.\n",
       ":param size: A 2-tuple, containing (width, height) in pixels.\n",
       ":param color: What color to use for the image.  Default is black.\n",
       "   If given, this should be a single integer or floating point value\n",
       "   for single-band modes, and a tuple for multi-band modes (one value\n",
       "   per band).  When creating RGB images, you can also use color\n",
       "   strings as supported by the ImageColor module.  If the color is\n",
       "   None, the image is not initialised.\n",
       ":returns: An :py:class:`~PIL.Image.Image` object.\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\user\\anaconda3\\lib\\site-packages\\pil\\image.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = Image.new?\n",
    "#(mode='L', size=(height, width), color=255)\n",
    "\n",
    "    # Draw some lines\n",
    "#draw = ImageDraw.Draw?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20., -1., -1., -1., -1., -1., -1.],\n",
       "       [-1., -1., -1., -1., -1., -1., -1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0., -1.],\n",
       "       [-1., -1., -1., -1., -1., -1., -1.],\n",
       "       [-1., -1., -1., -1., -1., -1., -1.],\n",
       "       [-1., -1., -1., -1., -1., -1., -1.],\n",
       "       [-1., -1., -1., -1., -1., -1., -1.]])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.zeros((7,7))-1\n",
    "x[0,0] = 20\n",
    "x[2,0:6] = 0\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = [[20., 19., 18., 17, 16, 15, 14],\n",
    "       [19, 18., 17, 16, 15, 14, 13],\n",
    "       [ 0.,  0.,  0.,  0.,  0.,  0., 12],\n",
    "       [5, 6, 7, 8, 9, 10, 11],\n",
    "       [4, 5, 6, 7, 8, 9, 10],\n",
    "       [3, 4, 5, 6, 7, 8, 9],\n",
    "       [2, 3, 4, 5, 6, 7, 8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_pad = np.pad(xx, 1, mode='constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0., 20., 19., 18., 17., 16., 15., 14.,  0.],\n",
       "       [ 0., 19., 18., 17., 16., 15., 14., 13.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0., 12.,  0.],\n",
       "       [ 0.,  5.,  6.,  7.,  8.,  9., 10., 11.,  0.],\n",
       "       [ 0.,  4.,  5.,  6.,  7.,  8.,  9., 10.,  0.],\n",
       "       [ 0.,  3.,  4.,  5.,  6.,  7.,  8.,  9.,  0.],\n",
       "       [ 0.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
